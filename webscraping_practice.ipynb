{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping\n",
    "\n",
    "In this exercise, you'll practice using BeautifulSoup to parse the content of a web page. The page that you'll be scraping, https://realpython.github.io/fake-jobs/, contains job listings. Your job is to extract the data on each job and convert into a pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import io\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Start by performing a GET request on the url above and convert the response into a BeautifulSoup object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'https://realpython.github.io/fake-jobs/'\n",
    "response = requests.get(endpoint)\n",
    "response.status_code\n",
    "soup = BeautifulSoup(response.text, features=\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### a. Use the .find method to find the tag containing the first job title (\"Senior Python Developer\"). Hint: can you find a tag type and/or a class that could be helpful for extracting this information? Extract the text from this title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_python_dev = soup.find('h2').text#:'Senior Python Developer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### b. Now, use what you did for the first title, but extract the job title for all jobs on this page. Store the results in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = [item.text for item in soup.findAll('h2')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### c. Finally, extract the companies, locations, and posting dates for each job. For example, the first job has a company of \"Payne, Roberts and Davis\", a location of \"Stewartbury, AA\", and a posting date of \"2021-04-08\". Ensure that the text that you extract is clean, meaning no extra spaces or other characters at the beginning or end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = [name.text for name in soup.findAll(attrs={'class':'subtitle is-6 company'})]\n",
    "locations = [place.text for place in soup.findAll(attrs={'class':'location'})]\n",
    "locations = [item.strip() for item in locations if str(item)]\n",
    "dates = [day.text for day in soup.findAll('time')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Take the lists that you have created and combine them into a pandas DataFrame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Python Developer</td>\n",
       "      <td>Payne, Roberts and Davis</td>\n",
       "      <td>Stewartbury, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Energy engineer</td>\n",
       "      <td>Vasquez-Davidson</td>\n",
       "      <td>Christopherville, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legal executive</td>\n",
       "      <td>Jackson, Chambers and Levy</td>\n",
       "      <td>Port Ericaburgh, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>Savage-Bradley</td>\n",
       "      <td>East Seanview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product manager</td>\n",
       "      <td>Ramirez Inc</td>\n",
       "      <td>North Jamieview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Museum/gallery exhibitions officer</td>\n",
       "      <td>Nguyen, Yoder and Petty</td>\n",
       "      <td>Lake Abigail, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Radiographer, diagnostic</td>\n",
       "      <td>Holder LLC</td>\n",
       "      <td>Jacobshire, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Database administrator</td>\n",
       "      <td>Yates-Ferguson</td>\n",
       "      <td>Port Susan, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Furniture designer</td>\n",
       "      <td>Ortega-Lawrence</td>\n",
       "      <td>North Tiffany, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ship broker</td>\n",
       "      <td>Fuentes, Walls and Castro</td>\n",
       "      <td>Michelleville, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             job title                     company  \\\n",
       "0              Senior Python Developer    Payne, Roberts and Davis   \n",
       "1                      Energy engineer            Vasquez-Davidson   \n",
       "2                      Legal executive  Jackson, Chambers and Levy   \n",
       "3               Fitness centre manager              Savage-Bradley   \n",
       "4                      Product manager                 Ramirez Inc   \n",
       "..                                 ...                         ...   \n",
       "95  Museum/gallery exhibitions officer     Nguyen, Yoder and Petty   \n",
       "96            Radiographer, diagnostic                  Holder LLC   \n",
       "97              Database administrator              Yates-Ferguson   \n",
       "98                  Furniture designer             Ortega-Lawrence   \n",
       "99                         Ship broker   Fuentes, Walls and Castro   \n",
       "\n",
       "                location        date  \n",
       "0        Stewartbury, AA  2021-04-08  \n",
       "1   Christopherville, AA  2021-04-08  \n",
       "2    Port Ericaburgh, AA  2021-04-08  \n",
       "3      East Seanview, AP  2021-04-08  \n",
       "4    North Jamieview, AP  2021-04-08  \n",
       "..                   ...         ...  \n",
       "95      Lake Abigail, AE  2021-04-08  \n",
       "96        Jacobshire, AP  2021-04-08  \n",
       "97        Port Susan, AE  2021-04-08  \n",
       "98     North Tiffany, AA  2021-04-08  \n",
       "99     Michelleville, AP  2021-04-08  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_app_data = pd.DataFrame({'job title': job_titles, 'company':companies,'location': locations, 'date':dates})\n",
    "job_app_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Next, add a column that contains the url for the \"Apply\" button. Try this in two ways.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. First, use the BeautifulSoup find_all method to extract the urls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/energy-engineer-1.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/legal-executive-2.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/fitness-centre-manager-3.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/product-manager-4.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/medical-technical-officer-5.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/physiological-scientist-6.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/textile-designer-7.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/waste-management-officer-9.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/software-engineer-python-10.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/interpreter-11.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/architect-12.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/meteorologist-13.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/audiological-scientist-14.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/english-as-a-second-language-teacher-15.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/surgeon-16.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/equities-trader-17.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/newspaper-journalist-18.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/materials-engineer-19.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/python-programmer-entry-level-20.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/product-process-development-scientist-21.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/scientist-research-maths-22.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/ecologist-23.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/materials-engineer-24.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/historic-buildings-inspector-conservation-officer-25.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/data-scientist-26.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/psychiatrist-27.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/structural-engineer-28.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/immigration-officer-29.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/python-programmer-entry-level-30.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/neurosurgeon-31.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/broadcast-engineer-32.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/make-33.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/nurse-adult-34.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/air-broker-35.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/editor-film-video-36.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/production-assistant-radio-37.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/engineer-communications-38.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/sales-executive-39.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/software-developer-python-40.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/futures-trader-41.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/tour-manager-42.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/cytogeneticist-43.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/designer-multimedia-44.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/trade-union-research-officer-45.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/chemist-analytical-46.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/programmer-multimedia-47.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/engineer-broadcasting-operations-48.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/teacher-primary-school-49.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/python-developer-50.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/manufacturing-systems-engineer-51.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/producer-television-film-video-52.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/scientist-forensic-53.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/bonds-trader-54.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/editorial-assistant-55.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/photographer-56.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/retail-banker-57.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/jewellery-designer-58.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/ophthalmologist-59.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/back-end-web-developer-python-django-60.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/licensed-conveyancer-61.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/futures-trader-62.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/counselling-psychologist-63.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/insurance-underwriter-64.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/engineer-automotive-65.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/producer-radio-66.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/dispensing-optician-67.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/designer-fashion-clothing-68.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/chartered-loss-adjuster-69.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/back-end-web-developer-python-django-70.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/forest-woodland-manager-71.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/clinical-cytogeneticist-72.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/print-production-planner-73.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/systems-developer-74.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/graphic-designer-75.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/writer-76.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/field-seismologist-77.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/chief-strategy-officer-78.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/air-cabin-crew-79.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/python-programmer-entry-level-80.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/warden-ranger-81.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/sports-therapist-82.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/arts-development-officer-83.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/printmaker-84.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/health-and-safety-adviser-85.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/manufacturing-systems-engineer-86.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/programmer-applications-87.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/medical-physicist-88.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/media-planner-89.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/software-developer-python-90.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/surveyor-land-geomatics-91.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/legal-executive-92.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/librarian-academic-93.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/barrister-94.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/museum-gallery-exhibitions-officer-95.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/radiographer-diagnostic-96.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/database-administrator-97.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/furniture-designer-98.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/ship-broker-99.html']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [x['href'] for x in soup.find_all('a') if x.text == 'Apply']\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Next, get those same urls in a different way. Examine the urls and see if you can spot the pattern of how they are constructed. Then, build the url using the elements you have already extracted. Ensure that the urls that you created match those that you extracted using BeautifulSoup. Warning: You will need to do some string cleaning and prep in constructing the urls this way. For example, look carefully at the urls for the \"Software Engineer (Python)\" job and the \"Scientist, research (maths)\" job.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Python Developer</td>\n",
       "      <td>Payne, Roberts and Davis</td>\n",
       "      <td>Stewartbury, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Energy engineer</td>\n",
       "      <td>Vasquez-Davidson</td>\n",
       "      <td>Christopherville, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legal executive</td>\n",
       "      <td>Jackson, Chambers and Levy</td>\n",
       "      <td>Port Ericaburgh, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>Savage-Bradley</td>\n",
       "      <td>East Seanview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product manager</td>\n",
       "      <td>Ramirez Inc</td>\n",
       "      <td>North Jamieview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Museum/gallery exhibitions officer</td>\n",
       "      <td>Nguyen, Yoder and Petty</td>\n",
       "      <td>Lake Abigail, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Radiographer, diagnostic</td>\n",
       "      <td>Holder LLC</td>\n",
       "      <td>Jacobshire, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Database administrator</td>\n",
       "      <td>Yates-Ferguson</td>\n",
       "      <td>Port Susan, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Furniture designer</td>\n",
       "      <td>Ortega-Lawrence</td>\n",
       "      <td>North Tiffany, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ship broker</td>\n",
       "      <td>Fuentes, Walls and Castro</td>\n",
       "      <td>Michelleville, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/sh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             job title                     company  \\\n",
       "0              Senior Python Developer    Payne, Roberts and Davis   \n",
       "1                      Energy engineer            Vasquez-Davidson   \n",
       "2                      Legal executive  Jackson, Chambers and Levy   \n",
       "3               Fitness centre manager              Savage-Bradley   \n",
       "4                      Product manager                 Ramirez Inc   \n",
       "..                                 ...                         ...   \n",
       "95  Museum/gallery exhibitions officer     Nguyen, Yoder and Petty   \n",
       "96            Radiographer, diagnostic                  Holder LLC   \n",
       "97              Database administrator              Yates-Ferguson   \n",
       "98                  Furniture designer             Ortega-Lawrence   \n",
       "99                         Ship broker   Fuentes, Walls and Castro   \n",
       "\n",
       "                location        date  \\\n",
       "0        Stewartbury, AA  2021-04-08   \n",
       "1   Christopherville, AA  2021-04-08   \n",
       "2    Port Ericaburgh, AA  2021-04-08   \n",
       "3      East Seanview, AP  2021-04-08   \n",
       "4    North Jamieview, AP  2021-04-08   \n",
       "..                   ...         ...   \n",
       "95      Lake Abigail, AE  2021-04-08   \n",
       "96        Jacobshire, AP  2021-04-08   \n",
       "97        Port Susan, AE  2021-04-08   \n",
       "98     North Tiffany, AA  2021-04-08   \n",
       "99     Michelleville, AP  2021-04-08   \n",
       "\n",
       "                                                  url  \n",
       "0   https://realpython.github.io/fake-jobs/jobs/se...  \n",
       "1   https://realpython.github.io/fake-jobs/jobs/en...  \n",
       "2   https://realpython.github.io/fake-jobs/jobs/le...  \n",
       "3   https://realpython.github.io/fake-jobs/jobs/fi...  \n",
       "4   https://realpython.github.io/fake-jobs/jobs/pr...  \n",
       "..                                                ...  \n",
       "95  https://realpython.github.io/fake-jobs/jobs/mu...  \n",
       "96  https://realpython.github.io/fake-jobs/jobs/ra...  \n",
       "97  https://realpython.github.io/fake-jobs/jobs/da...  \n",
       "98  https://realpython.github.io/fake-jobs/jobs/fu...  \n",
       "99  https://realpython.github.io/fake-jobs/jobs/sh...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_app_data['url_form'] = (\n",
    "    job_app_data['job title']\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[\\ !@#$%\\^&*\\(\\)\\[\\]\\{\\};:,\\.\\/<>?`~=_\\+]\", '-', regex=True)+'-'+job_app_data.index.astype('string')\n",
    ")\n",
    "job_app_data['url_form'] = [re.sub(r'--+', '-', row) for row in job_app_data['url_form']]\n",
    "job_app_data['url'] = 'https://realpython.github.io/fake-jobs/jobs/' + job_app_data['url_form'].astype('string') + '.html'\n",
    "job_app_data = job_app_data.drop(columns='url_form')\n",
    "job_app_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Finally, we want to get the job description text for each job.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Start by looking at the page for the first job, https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html. Using BeautifulSoup, extract the job description paragraph.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<html>\n",
       "<head>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
       "<title>Fake Python</title>\n",
       "<link href=\"https://cdn.jsdelivr.net/npm/bulma@0.9.2/css/bulma.min.css\" rel=\"stylesheet\"/>\n",
       "</head>\n",
       "<body>\n",
       "<section class=\"section\">\n",
       "<div class=\"container mb-5\">\n",
       "<h1 class=\"title is-1\">\n",
       "        Fake Python\n",
       "      </h1>\n",
       "<p class=\"subtitle is-3\">\n",
       "        Fake Jobs for Your Web Scraping Journey\n",
       "      </p>\n",
       "</div>\n",
       "<div class=\"container\">\n",
       "<div class=\"columns is-multiline\" id=\"ResultsContainer\">\n",
       "<div class=\"box\">\n",
       "<h1 class=\"title is-2\">Senior Python Developer</h1>\n",
       "<h2 class=\"subtitle is-4 company\">Payne, Roberts and Davis</h2>\n",
       "<div class=\"content\">\n",
       "<p>Professional asset web application environmentally friendly detail-oriented asset. Coordinate educational dashboard agile employ growth opportunity. Company programs CSS explore role. Html educational grit web application. Oversea SCRUM talented support. Web Application fast-growing communities inclusive programs job CSS. Css discussions growth opportunity explore open-minded oversee. Css Python environmentally friendly collaborate inclusive role. Django no experience oversee dashboard environmentally friendly willing to learn programs. Programs open-minded programs asset.</p>\n",
       "<p id=\"location\"><strong>Location:</strong> Stewartbury, AA</p>\n",
       "<p id=\"date\"><strong>Posted:</strong> 2021-04-08</p>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</section>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint = 'https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html'\n",
    "response2 = requests.get(endpoint)\n",
    "soup = BeautifulSoup(response2.text, features=\"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Professional asset web application environmentally friendly detail-oriented asset. Coordinate educational dashboard agile employ growth opportunity. Company programs CSS explore role. Html educational grit web application. Oversea SCRUM talented support. Web Application fast-growing communities inclusive programs job CSS. Css discussions growth opportunity explore open-minded oversee. Css Python environmentally friendly collaborate inclusive role. Django no experience oversee dashboard environmentally friendly willing to learn programs. Programs open-minded programs asset.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jd = soup.findAll('p')[1].text\n",
    "jd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. We want to be able to do this for all pages. Write a function which takes as input a url and returns the description text on that page. For example, if you input \"https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html\" into your function, it should return the string \"At be than always different American address. Former claim chance prevent why measure too. Almost before some military outside baby interview. Face top individual win suddenly. Parent do ten after those scientist. Medical effort assume teacher wall. Significant his himself clearly very. Expert stop area along individual. Three own bank recognize special good along.\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jd(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, features='html.parser')\n",
    "    jd = soup.findAll('p')[1].text\n",
    "    return jd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At be than always different American address. Former claim chance prevent why measure too. Almost before some military outside baby interview. Face top individual win suddenly. Parent do ten after those scientist. Medical effort assume teacher wall. Significant his himself clearly very. Expert stop area along individual. Three own bank recognize special good along.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html'\n",
    "get_jd(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Use the [.apply method](https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html) on the url column you created above to retrieve the description text for all of the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>job description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Python Developer</td>\n",
       "      <td>Payne, Roberts and Davis</td>\n",
       "      <td>Stewartbury, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/se...</td>\n",
       "      <td>Professional asset web application environment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Energy engineer</td>\n",
       "      <td>Vasquez-Davidson</td>\n",
       "      <td>Christopherville, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/en...</td>\n",
       "      <td>Party prevent live. Quickly candidate change a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legal executive</td>\n",
       "      <td>Jackson, Chambers and Levy</td>\n",
       "      <td>Port Ericaburgh, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/le...</td>\n",
       "      <td>Administration even relate head color. Staff b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>Savage-Bradley</td>\n",
       "      <td>East Seanview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/fi...</td>\n",
       "      <td>Tv program actually race tonight themselves tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product manager</td>\n",
       "      <td>Ramirez Inc</td>\n",
       "      <td>North Jamieview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/pr...</td>\n",
       "      <td>Traditional page a although for study anyone. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Museum/gallery exhibitions officer</td>\n",
       "      <td>Nguyen, Yoder and Petty</td>\n",
       "      <td>Lake Abigail, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/mu...</td>\n",
       "      <td>Paper age physical current note. There reality...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Radiographer, diagnostic</td>\n",
       "      <td>Holder LLC</td>\n",
       "      <td>Jacobshire, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/ra...</td>\n",
       "      <td>Able such right culture. Wrong pick structure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Database administrator</td>\n",
       "      <td>Yates-Ferguson</td>\n",
       "      <td>Port Susan, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/da...</td>\n",
       "      <td>Create day party decade high clear. Past trade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Furniture designer</td>\n",
       "      <td>Ortega-Lawrence</td>\n",
       "      <td>North Tiffany, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/fu...</td>\n",
       "      <td>Pressure under rock next week. Recognize so re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ship broker</td>\n",
       "      <td>Fuentes, Walls and Castro</td>\n",
       "      <td>Michelleville, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/sh...</td>\n",
       "      <td>Management common popular project only. Must s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             job title                     company  \\\n",
       "0              Senior Python Developer    Payne, Roberts and Davis   \n",
       "1                      Energy engineer            Vasquez-Davidson   \n",
       "2                      Legal executive  Jackson, Chambers and Levy   \n",
       "3               Fitness centre manager              Savage-Bradley   \n",
       "4                      Product manager                 Ramirez Inc   \n",
       "..                                 ...                         ...   \n",
       "95  Museum/gallery exhibitions officer     Nguyen, Yoder and Petty   \n",
       "96            Radiographer, diagnostic                  Holder LLC   \n",
       "97              Database administrator              Yates-Ferguson   \n",
       "98                  Furniture designer             Ortega-Lawrence   \n",
       "99                         Ship broker   Fuentes, Walls and Castro   \n",
       "\n",
       "                location        date  \\\n",
       "0        Stewartbury, AA  2021-04-08   \n",
       "1   Christopherville, AA  2021-04-08   \n",
       "2    Port Ericaburgh, AA  2021-04-08   \n",
       "3      East Seanview, AP  2021-04-08   \n",
       "4    North Jamieview, AP  2021-04-08   \n",
       "..                   ...         ...   \n",
       "95      Lake Abigail, AE  2021-04-08   \n",
       "96        Jacobshire, AP  2021-04-08   \n",
       "97        Port Susan, AE  2021-04-08   \n",
       "98     North Tiffany, AA  2021-04-08   \n",
       "99     Michelleville, AP  2021-04-08   \n",
       "\n",
       "                                                  url  \\\n",
       "0   https://realpython.github.io/fake-jobs/jobs/se...   \n",
       "1   https://realpython.github.io/fake-jobs/jobs/en...   \n",
       "2   https://realpython.github.io/fake-jobs/jobs/le...   \n",
       "3   https://realpython.github.io/fake-jobs/jobs/fi...   \n",
       "4   https://realpython.github.io/fake-jobs/jobs/pr...   \n",
       "..                                                ...   \n",
       "95  https://realpython.github.io/fake-jobs/jobs/mu...   \n",
       "96  https://realpython.github.io/fake-jobs/jobs/ra...   \n",
       "97  https://realpython.github.io/fake-jobs/jobs/da...   \n",
       "98  https://realpython.github.io/fake-jobs/jobs/fu...   \n",
       "99  https://realpython.github.io/fake-jobs/jobs/sh...   \n",
       "\n",
       "                                      job description  \n",
       "0   Professional asset web application environment...  \n",
       "1   Party prevent live. Quickly candidate change a...  \n",
       "2   Administration even relate head color. Staff b...  \n",
       "3   Tv program actually race tonight themselves tr...  \n",
       "4   Traditional page a although for study anyone. ...  \n",
       "..                                                ...  \n",
       "95  Paper age physical current note. There reality...  \n",
       "96  Able such right culture. Wrong pick structure ...  \n",
       "97  Create day party decade high clear. Past trade...  \n",
       "98  Pressure under rock next week. Recognize so re...  \n",
       "99  Management common popular project only. Must s...  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_app_data['job description'] = job_app_data['url'].apply(get_jd)\n",
    "job_app_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Navigate to https://www.billboard.com/charts/hot-100/. Using BeautifulSoup, extract out the This Week, artist, song, Last Week, Peak Position, and Weeks on Chart values into a pandas DataFrame. Hint: The HTML for the number one ranked song is slightly different from that of the rest of the songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'https://www.billboard.com/charts/hot-100/'\n",
    "response = requests.get(endpoint)\n",
    "soup = BeautifulSoup(response.text, features='html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The below commented out code was my first attempt at pulling each requested item by row, but I ran into indexing issues. I then backed out and realized i could do a larger for loop that would give my all of the information i was looking for except the 'This Week' column, because that was in a separate div and fell outside the scope of the for loop. However, the this_week list comprehension worked great, but it's also in index order more or less, so I could use the index as the 'This Week' column if I set the first value to 1 instead of 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles = [row.h3.text.strip() for row in soup.select('.o-chart-results-list-row')]\n",
    "# this_week = [row.span.text.strip() for row in soup.select('.o-chart-results-list-row')]\n",
    "# artists = [row.ul.li.span.text.strip() for row in soup.select('.o-chart-results-list-row')]\n",
    "# last_week = [row.ul.ul.span.text.strip() for row in soup.select('.o-chart-results-list-row')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is my final solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song Title</th>\n",
       "      <th>Artist</th>\n",
       "      <th>This Week</th>\n",
       "      <th>Last Week</th>\n",
       "      <th>Peak</th>\n",
       "      <th>Weeks on Chart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love Somebody</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>1</td>\n",
       "      <td>New to the list!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Bar Song (Tipsy)</td>\n",
       "      <td>Shaboozey</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Birds Of A Feather</td>\n",
       "      <td>Billie Eilish</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Die With A Smile</td>\n",
       "      <td>Lady Gaga &amp; Bruno Mars</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Espresso</td>\n",
       "      <td>Sabrina Carpenter</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Leave Me Alone</td>\n",
       "      <td>BigXthaPlug</td>\n",
       "      <td>96</td>\n",
       "      <td>99</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Belong Together</td>\n",
       "      <td>Mark Ambor</td>\n",
       "      <td>97</td>\n",
       "      <td>New to the list!</td>\n",
       "      <td>74</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>The Emptiness Machine</td>\n",
       "      <td>Linkin Park</td>\n",
       "      <td>98</td>\n",
       "      <td>New to the list!</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Mantra</td>\n",
       "      <td>Jennie</td>\n",
       "      <td>99</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Angel With An Attitude</td>\n",
       "      <td>Rod Wave</td>\n",
       "      <td>100</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Song Title                  Artist This Week  \\\n",
       "0            Love Somebody           Morgan Wallen         1   \n",
       "1       A Bar Song (Tipsy)               Shaboozey         2   \n",
       "2       Birds Of A Feather           Billie Eilish         3   \n",
       "3         Die With A Smile  Lady Gaga & Bruno Mars         4   \n",
       "4                 Espresso       Sabrina Carpenter         5   \n",
       "..                     ...                     ...       ...   \n",
       "95          Leave Me Alone             BigXthaPlug        96   \n",
       "96         Belong Together              Mark Ambor        97   \n",
       "97   The Emptiness Machine             Linkin Park        98   \n",
       "98                  Mantra                  Jennie        99   \n",
       "99  Angel With An Attitude                Rod Wave       100   \n",
       "\n",
       "           Last Week Peak Weeks on Chart  \n",
       "0   New to the list!    1              1  \n",
       "1                  1    1             28  \n",
       "2                  2    2             23  \n",
       "3                  4    3             10  \n",
       "4                  3    3             28  \n",
       "..               ...  ...            ...  \n",
       "95                99   96              2  \n",
       "96  New to the list!   74             24  \n",
       "97  New to the list!   21              6  \n",
       "98                98   98              2  \n",
       "99                46   46              2  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list comprehension to get 'This Week' values\n",
    "this_week = [row.span.text.strip() for row in soup.select('.o-chart-results-list-row')]\n",
    "# initialize list for use inside loop\n",
    "here = []\n",
    "for row in soup.select('.o-chart-results-list-row'):\n",
    "    item = row.ul.text.strip() # gets the text info inside the unordered list\n",
    "    item = re.sub(r'[\\n\\t]+', ',', item) # removes many \\n and \\t and replaces with commas\n",
    "    here.append(item) #add each item to the list\n",
    "\n",
    "# split the list into elements separated by commas rather than a list of 100 elements\n",
    "data = [row.split(',') for row in here]\n",
    "# create dataframe, clean, and reshape\n",
    "info_df = (\n",
    "    pd.DataFrame(data,columns = ['Song Title', 'Artist', 'Last Week A', 'Peak A', 'Wks on Chart A','Last Week B', 'Peak B', 'Wks on Chart B', 'None' ])\n",
    "    .drop(columns=['Last Week B', 'Peak B', 'Wks on Chart B', 'None'])\n",
    "    .rename(columns={'Last Week A':'Last Week', 'Peak A':'Peak', 'Wks on Chart A':'Weeks on Chart'})\n",
    ")\n",
    "info_df['This Week'] = this_week\n",
    "info_df['Last Week'] = [re.sub(r'-+', 'New to the list!', row) for row in info_df['Last Week']]\n",
    "info_df = info_df.loc[:,['Song Title', 'Artist','This Week', 'Last Week', 'Peak', 'Weeks on Chart']]\n",
    "info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. After getting the code working for the current chart, navigate to last week's chart. Notice how the url for the page changes. Write a function which will, given a date, return a pandas DataFrame containing the Billboard chart data for that date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Link format**\n",
    "https://www.billboard.com/charts/hot-100/2024-10-26/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_billboard_chart(date=None):\n",
    "#     if date == None:\n",
    "#         date = datetime.date.today().strftime('%Y-%m-%d')\n",
    "#     endpoint = 'https://www.billboard.com/charts/hot-100/'+date+'/'\n",
    "#     response = requests.get(endpoint)\n",
    "#     soup = BeautifulSoup(response.text, features='html.parser')\n",
    "#     this_week = [row.span.text.strip() for row in soup.select('.o-chart-results-list-row')]\n",
    "#     here = []\n",
    "#     for row in soup.select('.o-chart-results-list-row'):\n",
    "#         item = row.ul.text.strip() # gets the text info inside the unordered list\n",
    "#         item = re.sub(r'[\\n\\t]+', ',', item) # removes many \\n and \\t and replaces with commas\n",
    "#         here.append(item) #add each item to the list\n",
    "#     # split the list into elements separated by commas rather than a list of 100 elements\n",
    "#     data = [row.split(',') for row in here]\n",
    "#     info_df = (\n",
    "#     pd.DataFrame(data,columns = ['Song Title', 'Artist', 'Last Week A', 'Peak A', 'Wks on Chart A','Last Week B', 'Peak B', 'Wks on Chart B', 'None' ])\n",
    "#     .drop(columns=['Last Week B', 'Peak B', 'Wks on Chart B', 'None'])\n",
    "#     .rename(columns={'Last Week A':'Last Week', 'Peak A':'Peak', 'Wks on Chart A':'Weeks on Chart'})\n",
    "#     )\n",
    "#     info_df['This Week'] = this_week\n",
    "#     info_df['Last Week'] = [re.sub(r'-+', 'New to the list!', row) for row in info_df['Last Week']]\n",
    "#     info_df = info_df.loc[:,['Song Title', 'Artist','This Week', 'Last Week', 'Peak', 'Weeks on Chart']]\n",
    "#     return info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "9 columns passed, passed data had 10 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    989\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 9 columns passed, passed data had 10 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-10-17\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m get_billboard_chart(date)\n",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m, in \u001b[0;36mget_billboard_chart\u001b[0;34m(date)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# split the list into elements separated by commas rather than a list of 100 elements\u001b[39;00m\n\u001b[1;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m [row\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m here]\n\u001b[1;32m     15\u001b[0m info_df \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 16\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(data,columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSong Title\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArtist\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLast Week A\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeak A\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWks on Chart A\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLast Week B\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeak B\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWks on Chart B\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m ])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLast Week B\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeak B\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWks on Chart B\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLast Week A\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLast Week\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeak A\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeak\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWks on Chart A\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeeks on Chart\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m info_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis Week\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m this_week\n\u001b[1;32m     21\u001b[0m info_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLast Week\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNew to the list!\u001b[39m\u001b[38;5;124m'\u001b[39m, row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m info_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLast Week\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[1;32m    852\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[1;32m    853\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m    854\u001b[0m         data,\n\u001b[1;32m    855\u001b[0m         columns,\n\u001b[1;32m    856\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    857\u001b[0m         dtype,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    860\u001b[0m         arrays,\n\u001b[1;32m    861\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    865\u001b[0m     )\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 9 columns passed, passed data had 10 columns"
     ]
    }
   ],
   "source": [
    "# date = '2024-10-17'\n",
    "# get_billboard_chart(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write a loop to retrieve the Billboard chart data for the last 10 weeks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
